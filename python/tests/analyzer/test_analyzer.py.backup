
"""
Tests for Strategic Site Analyzer
"""

import pytest
import json
import jsonschema
from pathlib import Path
import asyncio
from unittest.mock import Mock, patch, AsyncMock
import aiohttp

from orion.analyzer.analyzer import SiteAnalyzer
from orion.config import Config as OrionConfig


class TestSiteAnalyzer:
    """Test cases for SiteAnalyzer class."""
    
    @pytest.fixture
    def config(self):
        """Mock OrionConfig for testing."""
        config = Mock(spec=OrionConfig)
        config.api_url = "http://localhost:8000"
        config.api_key = "test_key"
        return config
    
    @pytest.fixture

    @pytest.fixture
    def schema(self):
        """Load JSON schema for validation."""
        schema_path = Path(__file__).parent.parent.parent / "orion" / "analyzer" / "schema.json"
        with open(schema_path, "r") as f:
            return json.load(f)
        with open(schema_path, 'r') as f:
            return json.load(f)
        with open(schema_path, 'r') as f:
            return json.load(f)
        """SiteAnalyzer instance for testing."""
        with patch('orion.analyzer.analyzer.OrionAPIClient'):
            return SiteAnalyzer(config)
    
    @pytest.mark.asyncio
    async def test_analyze_my_site_mode(self, analyzer):
        """Test my-site analysis mode."""
        with patch.object(analyzer, '_analyze_my_site', new_callable=AsyncMock) as mock_analyze:
            mock_analyze.return_value = {
                'mode': 'my-site',
                'target_url': 'https://example.com',
                'analysis': {'seo_analysis': {'title_seo_score': 8.5}}
            }
            
            result = await analyzer.analyze('my-site', 'https://example.com')
            
            assert result['mode'] == 'my-site'
            assert result['target_url'] == 'https://example.com'
            mock_analyze.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_analyze_competitor_mode(self, analyzer):
        """Test competitor analysis mode."""
        with patch.object(analyzer, '_analyze_competitor', new_callable=AsyncMock) as mock_analyze:
            mock_analyze.return_value = {
                'mode': 'competitor',
                'target_url': 'https://competitor.com',
                'analysis': {'content_strategy': {'content_quality_score': 7.5}}
            }
            
            result = await analyzer.analyze('competitor', 'https://competitor.com')
            
            assert result['mode'] == 'competitor'
            assert result['target_url'] == 'https://competitor.com'
            mock_analyze.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_analyze_prospect_mode(self, analyzer):
        """Test prospect analysis mode."""
        with patch.object(analyzer, '_analyze_prospect', new_callable=AsyncMock) as mock_analyze:
            mock_analyze.return_value = {
                'mode': 'prospect',
                'target_url': 'https://prospect.com',
                'analysis': {'authority_metrics': {'estimated_domain_authority': 65}}
            }
            
            result = await analyzer.analyze('prospect', 'https://prospect.com')
            
            assert result['mode'] == 'prospect'
            assert result['target_url'] == 'https://prospect.com'
            mock_analyze.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_analyze_invalid_mode(self, analyzer):
        """Test invalid analysis mode raises ValueError."""
        with pytest.raises(ValueError, match="Invalid analysis mode: invalid"):
            await analyzer.analyze('invalid', 'https://example.com')
    
    def test_extract_headings(self, analyzer):
        """Test heading extraction."""
        html_content = '''
        <h1>Main Title</h1>
        <h2>Subtitle</h2>
        <h3>Sub-subtitle</h3>
        <p>Regular content</p>
        <h2>Another Subtitle</h2>
        '''
        
        headings = analyzer._extract_headings(html_content)
        
        assert len(headings) == 4
        assert headings[0]['level'] == 1
        assert headings[0]['text'] == 'Main Title'
        assert headings[1]['level'] == 2
        assert headings[1]['text'] == 'Subtitle'
    
    def test_score_title_seo(self, analyzer):
        """Test title SEO scoring."""
        # Good title
        good_title = "Best Practices for Content Marketing Strategy"
        score = analyzer._score_title_seo(good_title)
        assert score >= 8.0
        
        # Short title
        short_title = "Short"
        score = analyzer._score_title_seo(short_title)
        assert score <= 8.0
        
        # Empty title
        empty_title = ""
        score = analyzer._score_title_seo(empty_title)
        assert score == 0.0
    
    def test_score_meta_seo(self, analyzer):
        """Test meta description SEO scoring."""
        # Good meta description
        good_meta = "Learn the best practices for content marketing strategy including keyword research, content planning, and performance measurement."
        score = analyzer._score_meta_seo(good_meta)
        assert score >= 8.0
        
        # Short meta description
        short_meta = "Short description"
        score = analyzer._score_meta_seo(short_meta)
        assert score <= 8.0
        
        # Empty meta description
        empty_meta = ""
        score = analyzer._score_meta_seo(empty_meta)
        assert score == 0.0
    
    def test_calculate_performance_score(self, analyzer):
        """Test performance score calculation."""
        # Good performance
        score = analyzer._calculate_performance_score(0.5, 100000)  # 0.5s, 100KB
        assert score >= 9.0
        
        # Poor performance
        score = analyzer._calculate_performance_score(5.0, 10000000)  # 5s, 10MB
        assert score < 6.0


# Run tests if executed directly
if __name__ == "__main__":
    pytest.main([__file__])

    @pytest.mark.asyncio
    async def test_my_site_required_sections(self, analyzer):
        """Test that my-site analysis includes all required sections."""
        with patch('aiohttp.ClientSession') as mock_session:
            mock_response = AsyncMock()
            mock_response.text.return_value = "<html><head><title>Test</title></head><body><h1>Test</h1></body></html>"
            mock_response.headers = {"title": "Test"}
            mock_session.return_value.__aenter__.return_value.get.return_value = mock_response
            
            result = await analyzer._analyze_my_site("https://example.com", {"max_pages": 5, "use_llm": False})
            
            # Check required sections exist
            assert 'seo_audit' in result['analysis']
            assert 'site_blueprint' in result['analysis']
            assert 'linking_opportunities' in result['analysis']
            assert result['analysis']['seo_audit'] is not None
            assert result['analysis']['site_blueprint'] is not None
            assert result['analysis']['linking_opportunities'] is not None
    
    @pytest.mark.asyncio
    async def test_competitor_required_sections(self, analyzer):
        """Test that competitor analysis includes all required sections."""
        with patch('aiohttp.ClientSession') as mock_session:
            mock_response = AsyncMock()
            mock_response.text.return_value = "<html><head><title>Competitor</title></head><body><h1>Content</h1></body></html>"
            mock_response.headers = {"title": "Competitor"}
            mock_session.return_value.__aenter__.return_value.get.return_value = mock_response
            
            result = await analyzer._analyze_competitor("https://competitor.com", {"max_pages": 5, "use_llm": False})
            
            # Check required sections exist
            assert 'content_strategy' in result['analysis']
            assert 'keyword_strategy' in result['analysis']
            assert 'strategic_opportunities' in result['analysis']
            assert result['analysis']['content_strategy'] is not None
            assert result['analysis']['keyword_strategy'] is not None
            assert result['analysis']['strategic_opportunities'] is not None
    
    @pytest.mark.asyncio
    async def test_prospect_required_sections(self, analyzer):
        """Test that prospect analysis includes all required sections."""
        with patch('aiohttp.ClientSession') as mock_session:
            mock_response = AsyncMock()
            mock_response.text.return_value = "<html><head><title>Prospect</title></head><body><h1>Business</h1></body></html>"
            mock_response.headers = {"title": "Prospect"}
            mock_session.return_value.__aenter__.return_value.get.return_value = mock_response
            
            result = await analyzer._analyze_prospect("https://prospect.com", {"max_pages": 5, "use_llm": False})
            
            # Check required sections exist
            assert 'missed_opportunities_audit' in result['analysis']
            assert 'proposal' in result['analysis']
            assert 'business_impact_analysis' in result['analysis']
            assert result['analysis']['missed_opportunities_audit'] is not None
            assert result['analysis']['proposal'] is not None
            assert result['analysis']['business_impact_analysis'] is not None
    
    @pytest.mark.asyncio
    async def test_metadata_consistency(self, analyzer):
        """Test that all analysis modes include required metadata."""
        with patch('aiohttp.ClientSession') as mock_session:
            mock_response = AsyncMock()
            mock_response.text.return_value = "<html><head><title>Test</title></head><body><h1>Test</h1></body></html>"
            mock_response.headers = {"title": "Test"}
            mock_session.return_value.__aenter__.return_value.get.return_value = mock_response
            
            for mode in ['my-site', 'competitor', 'prospect']:
                result = await analyzer.analyze(mode, "https://example.com", max_pages=3, use_llm=False)
                
                # Check required metadata fields
                assert 'metadata' in result
                metadata = result['metadata']
                assert 'model' in metadata
                assert 'tokens_used' in metadata
                assert 'latency_ms' in metadata
                assert 'cost_usd' in metadata
                assert isinstance(metadata['latency_ms'], int)
                assert isinstance(metadata['cost_usd'], (int, float))
                assert metadata['latency_ms'] >= 0
                assert metadata['cost_usd'] >= 0
    
    @pytest.mark.asyncio
    async def test_json_schema_validation(self, analyzer, schema):
        """Test that analyzer output validates against JSON schema."""
        with patch('aiohttp.ClientSession') as mock_session:
            mock_response = AsyncMock()
            mock_response.text.return_value = "<html><head><title>Test</title></head><body><h1>Test</h1></body></html>"
            mock_response.headers = {"title": "Test"}
            mock_session.return_value.__aenter__.return_value.get.return_value = mock_response
            
            for mode in ['my-site', 'competitor', 'prospect']:
                result = await analyzer.analyze(mode, "https://example.com", max_pages=3, use_llm=False)
                
                # Validate against schema - should not raise exception
                jsonschema.validate(result, schema)
                
                # Additional validation for required fields
                assert result['mode'] == mode
                assert 'target_url' in result
                assert 'timestamp' in result
                assert 'analysis' in result
                assert 'recommendations' in result
                assert 'summary' in result
                assert 'metadata' in result
